import os
import numpy as np
import pybullet as p
from tqdm import tqdm
import time
import math
import torch
from PIL import Image
from transformers import LlamaTokenizer, LlamaForCausalLM, ViltProcessor, ViltForQuestionAnswering, AutoTokenizer, AutoModelForCausalLM

from env import ClutteredPushGrasp
from robot import UR5Robotiq85
from utilities import YCBModels, Camera

from huggingface_hub import login


# Log in with your token
login(token="accss_token", add_to_git_credential=True)
# Load LLaMA model
# Load model directly
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-8B")
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B")

# Load VLIT model
vilt_processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
vilt_model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")

def generate_arm_instructions(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=100)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def analyze_scene(image, question):
    inputs = vilt_processor(image, question, return_tensors="pt")
    outputs = vlit_model(**inputs)
    logits = outputs.logits
    predicted_answer = logits.argmax(-1).item()
    return vilt_processor.decode(predicted_answer)

def execute_arm_instructions(env, instructions):
    actions = instructions.split(', ')
    for action in actions:
        if 'move' in action:
            coords = action.split('to ')[1].strip('()').split(',')
            x, y, z = map(float, coords)
            # Assuming env.step() can handle position control
            obs, reward, done, info = env.step([x, y, z, 1], 'end')  # 1 for gripper open
        elif 'grip' in action:
            if 'open' in action:
                obs, reward, done, info = env.step([0, 0, 0, 1], 'end')  # Open gripper
            elif 'close' in action:
                obs, reward, done, info = env.step([0, 0, 0, 0], 'end')  # Close gripper
        time.sleep(0.1)  # Add a small delay to allow for visualization

def llama_vlit_control_demo():
    ycb_models = YCBModels(
        os.path.join('./data/ycb', '**', 'textured-decmp.obj'),
    )
    camera = Camera((1, 1, 1),
                    (0, 0, 0),
                    (0, 0, 1),
                    0.1, 5, (320, 320), 40)
    robot = UR5Robotiq85((0, 0.5, 0), (0, 0, 0))
    env = ClutteredPushGrasp(robot, ycb_models, camera, vis=True)
    env.reset()

    while True:
        # Capture image from the environment
        rgb, depth, seg = env.render_camera()
        scene_image = Image.fromarray(rgb)
        
        # Analyze the scene
        scene_description = analyze_scene(scene_image, "What objects are in the scene?")
        
        # Generate instructions for the robot arm
        prompt = f"Given the scene: {scene_description}, provide step-by-step instructions for the UR5 robot to pick up an object. Use actions like 'move to (x,y,z)', 'open grip', 'close grip'."
        arm_instructions = generate_arm_instructions(prompt)
        
        print(f"Generated instructions: {arm_instructions}")
        
        # Execute the instructions
        execute_arm_instructions(env, arm_instructions)
        
        # Get feedback
        rgb, depth, seg = env.render_camera()
        feedback_image = Image.fromarray(rgb)
        feedback = analyze_scene(feedback_image, "Was the object successfully picked up?")
        
        print(f"Feedback: {feedback}")
        
        if input("Continue? (y/n): ").lower() != 'y':
            break

    env.close()

if __name__ == '__main__':
    llama_vlit_control_demo()
