import pybullet as p
import pybullet_data
import time
import numpy as np
from PIL import Image
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPProcessor, CLIPModel

# PyBullet setup (as before)
physicsClient = p.connect(p.GUI)
p.setAdditionalSearchPath(pybullet_data.getDataPath())
p.setGravity(0, 0, -9.81)

planeId = p.loadURDF("plane.urdf")
tableId = p.loadURDF("table/table.urdf", [0, 0, 0])
robotStartPos = [0, 0, 0.6]
robotStartOrientation = p.getQuaternionFromEuler([0, 0, 0])
robotId = p.loadURDF("kuka_iiwa/model.urdf", robotStartPos, robotStartOrientation, useFixedBase=True)

# Set up LLaMA model for generating actions and descriptions
llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3.1-8B-Instruct")
llama_model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3.1-8B-Instruct", torch_dtype=torch.float16, device_map="auto")

# Set up CLIP model for scene validation
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Helper function to generate robot actions using LLaMA
def generate_robot_action(input_text):
    inputs = llama_tokenizer(input_text, return_tensors="pt").to(llama_model.device)
    outputs = llama_model.generate(**inputs, max_length=100)
    return llama_tokenizer.decode(outputs[0], skip_special_tokens=True)

# New helper function to generate task validation descriptions using LLaMA
def generate_task_descriptions(task_text):
    prompt = f"Generate possible descriptions for: {task_text}"
    inputs = llama_tokenizer(prompt, return_tensors="pt").to(llama_model.device)
    outputs = llama_model.generate(**inputs, max_length=100)
    return llama_tokenizer.decode(outputs[0], skip_special_tokens=True).split(". ")

# Simplified action parsing logic
def parse_action(action_text):
    if "move" in action_text.lower():
        return "move", [0.1, 0.1, 0.1]  # Example: move to a fixed position
    elif "grab" in action_text.lower():
        return "grab", None
    elif "release" in action_text.lower():
        return "release", None
    else:
        return "unknown", None

def execute_action(action, params):
    if action == "move":
        # Implement robot arm movement in PyBullet
        p.setJointMotorControlArray(robotId, range(p.getNumJoints(robotId)), 
                                    p.POSITION_CONTROL, targetPositions=params)
    elif action == "grab":
        # Implement grabbing logic
        print("Grabbing object")
    elif action == "release":
        # Implement releasing logic
        print("Releasing object")
    else:
        print("Unknown action")

# Capture image from PyBullet environment
def get_camera_image():
    width, height = 320, 240
    view_matrix = p.computeViewMatrix([0, 0, 2], [0, 0, 0], [1, 0, 0])
    proj_matrix = p.computeProjectionMatrixFOV(60, width/height, 0.1, 100)
    _, _, rgba, _, _ = p.getCameraImage(width, height, view_matrix, proj_matrix)
    rgb_array = np.array(rgba)[:, :, :3]
    return Image.fromarray(rgb_array)

# Analyze scene with CLIP using multiple descriptions
def analyze_scene_with_clip(image, descriptions):
    probabilities = []
    for description in descriptions:
        inputs = clip_processor(text=description, images=image, return_tensors="pt", padding=True)
        outputs = clip_model(**inputs)
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1)
        probabilities.append(probs.item())
    
    # Return the highest probability across descriptions
    return max(probabilities)

# Main loop
for _ in range(1000):
    user_input = input("Enter a command for the robot: ")
    
    # Generate robot action using LLaMA
    llm_output = generate_robot_action(user_input)
    print("LLM output:", llm_output)
    
    # Parse and execute action
    action, params = parse_action(llm_output)
    execute_action(action, params)
    
    # Step the simulation
    p.stepSimulation()
    time.sleep(1./240.)

    # Get visual feedback from PyBullet
    current_image = get_camera_image()
    
    # Generate multiple task descriptions for validation
    task_descriptions = generate_task_descriptions(user_input)
    print("Generated descriptions:", task_descriptions)
    
    # Analyze scene with CLIP using generated descriptions
    completion_probability = analyze_scene_with_clip(current_image, task_descriptions)
    
    print(f"Task completion probability: {completion_probability:.2f}")

    if completion_probability > 0.7:  # Adjust this threshold as needed
        print("Task appears to be completed successfully!")
    else:
        print("Task may not be completed yet.")

p.disconnect()
